{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-mail Spam Classification\n",
    "\n",
    "*Team PythonPcz (pronounce Python Packs): Zhipeng Mei, Chanon Chantaduly, Patrapee Pongtana*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project description:\n",
    "\n",
    "The dataset included for this project is based on a subset of the [SpamAssassin Public Corpus](http://spamassassin.apache.org/old/). Upper image of Figure 1 shows a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to “normalize’ these values’, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the email with the unique string “httpaddr” to indicate that a URL was present. This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "\n",
    "We have already implemented the following email preprocessing steps: lower- casing; removal of HTML tags; normalization of URLs, email addresses, and numbers. In addition, words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Finally, we removed all non-words and punctuation. The result of these preprocessing steps is shown in lower image of Figure 1.\n",
    "\n",
    "### Dataset:\n",
    "We have provided you with two files: spam train.txt, spam test.txt. Each row of the data files corresponds to a single email. The data can be downloaded from [this link](https://www.dropbox.com/sh/q7051ab9pef7979/AACoUjSQjLLUWSqdVd54R2Kva?dl=0). The first column gives the label (1=spam,0=not spam).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments:\n",
    "* This project will involve your implementing classification algorithms. Before you can build these models and measures their performance, split your training data (i.e. spam train.txt) into a training and validate set, putting the last 1000 emails into the validation set. Thus, you will have a new training set with 4000 emails and a validation set with 1000 emails. **Explain why measuring the performance of your final classifier would be problematic had you not created this validation set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ans: In a classification work flow, training data set are in two categories (training and validation). Then the test data set is for testing. The problem with not creating a validation set can cause the performance measure to perform poorly due to inaccurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transform all of the data into **feature vectors**. Build a vocabulary list using only the 4000 email training set by finding all words that occur across the training set. Note that we assume that the data in the validation and testsets is completely unseen when we train our model, and thus we do not use any information contained in them. Ignore all words that appear in fewer than X = 30 emails of the 4000 email training set. This is both a means of preventing overfitting and of improving scalability. For each email, transform it into a feature vector x where the ith entry, xi, is 1 if the ith word in the vocabulary occurs in the email, and 0 otherwise.\n",
    "\n",
    "* Train the linear classifier such as Naive Bayes using your training set. **How many mistakes are made before the algorithm terminates? Next, classify the emails in your validation set. What is the validation error? Explain your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ans: The algorithm makes about 1.575% mistake during the training phase. In addidtion, validation error is about 2.7% while testing the validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Explore some other algorithms to solve spam filter problem. And demon- strate your thoughts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def email_spam_classifier(training_file, testing_file):\n",
    "    \n",
    "    # ----------------------    \n",
    "    # --- training phase ---\n",
    "    # ----------------------\n",
    "    df = pd.read_csv(training_file, sep='\\n', header=None, names=['message'])\n",
    "    df['label'] = df['message'].str.split(' ').str[0]\n",
    "    # print(df.head())\n",
    "\n",
    "    # 0 is ham, 1 is spam\n",
    "    # print(df['label'].value_counts())\n",
    "\n",
    "    df_x = df[\"message\"] #all email content\n",
    "    df_y = df[\"label\"]   #spam/ham label in 1/0 as int\n",
    "\n",
    "    #split the 4000 (.8) emails for training, 1000 (.2) for validating\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size = .2, random_state = 42)\n",
    " \n",
    "    #transform words into int with Countvectorizer\n",
    "    cv = CountVectorizer()\n",
    "    X_traincv = cv.fit_transform(X_train)\n",
    "    X_testcv = cv.transform(X_test)\n",
    "    \n",
    "    #produce an int array from y_train\n",
    "    y_train = y_train.astype(int)\n",
    "    \n",
    "    #call the classifier from sklearn library\n",
    "    mnb = MultinomialNB()\n",
    "    #train the model\n",
    "    mnb.fit(X_traincv, y_train)\n",
    "    \n",
    "    pred_train = mnb.predict(X_traincv)\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    count_train=0\n",
    "    for i in range(len(pred_train)):\n",
    "        if pred_train[i] == y_train[i]: #compare validation predicion vs y_test reuslt\n",
    "            count_train=count_train+1\n",
    "    print(\"The training's accuracy is: \",count_train/len(pred_train)) \n",
    "    \n",
    "\n",
    "    # ------------------------\n",
    "    # --- Validating phase ---\n",
    "    # ------------------------\n",
    "    #validating the model\n",
    "    pred = mnb.predict(X_testcv)\n",
    "    \n",
    "    #produce an int array from y_train\n",
    "    y_test = np.array(y_test).astype(int) \n",
    "\n",
    "    count=0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == y_test[i]: #compare validation predicion vs y_test reuslt\n",
    "            count=count+1\n",
    "    #calculate the accuracy of the classifier's prediction\n",
    "    print(\"The validating's accuracy is: \",count/len(pred)) \n",
    "\n",
    "\n",
    "    # ---------------------\n",
    "    # --- testing phase ---\n",
    "    # ---------------------\n",
    "    test_raw_data = pd.read_csv(testing_file, sep='\\n', header=None, names=['message_test'])\n",
    "    test_raw_data['label_test'] = test_raw_data['message_test'].str.split(' ').str[0]\n",
    "    test_X = test_raw_data[\"message_test\"] #all email content\n",
    "    test_y = test_raw_data[\"label_test\"]   #spam/ham label in 1/0 format\n",
    "    test_X_cv = cv.transform(test_X)\n",
    "    pred_test = mnb.predict(test_X_cv)\n",
    "    test_y = np.array(test_y).astype(int)\n",
    "\n",
    "    count_test=0\n",
    "    for i in range(len(pred_test)):\n",
    "        if pred_test[i] == test_y[i]: #compare validation predicion vs y_test reuslt\n",
    "            count_test=count_test+1\n",
    "    print(\"The testing's accuracy is: \",count_test/len(pred_test)) #calculate the accuracy of the classifier's prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Execution Code Below for MNB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"spam_train.txt\"\n",
    "testing_file = \"spam_test.txt\"\n",
    "email_spam_classifier(training_file, testing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
